# -*- coding: utf-8 -*-
"""Untitled3 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pck9zxvPCpJd495AbmMr50nICCVG9ljS

# **Conflict Histories** - **Text Classification**

## Data Loading
"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical
from keras.callbacks import EarlyStopping
from keras.layers import Dropout
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
import re
import pickle
import nltk
import ssl
from hebb import Hebbian

from keras import backend as K
from keras.engine.topology import Layer

import numpy as np
import tensorflow as tf
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


'''
Pyth
root director pip install -r requirement .txt
He wants it locally
Explain each step, vectorizing balancing and feature 
app.py

nltk.download()
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk import word_tokenize
'''
# To store dataset in a Pandas Dataframe
import io
df = pd.read_csv('All_events_final.csv')
df.info()

# Chooose a subset, for practice
sample_df = df[0:10000]
#sample_df

#Checking for nulls
sample_df['COMMENT'].isnull().sum()

# Sanitize text if text is NaN. -Removing nulls
sample_df = sample_df[sample_df['COMMENT'].notnull()].reset_index(drop=True)

## Text preprocessing variables
### We will use unigram representation of tokens, which is a bag-of-words approach.
'''
import pprint
pp = pprint.PrettyPrinter(indent=4)
example = sample_df['COMMENT'][0]
'''
# We want to filter out:
# 1. stopwords (words or expressions that occur so often that they don't provide additional information. WATCH OUT: make sure that you don't mistakenly filter out stopwords you actually want to keep, especially negations!!)
# 2. Punctuation
# 3. Digits (very hard to understand for a machine in context)
# 3. Transform every word into its lemmatized version ('went' to 'go')

space = re.compile('[/(){}\[\]\|@,;]')
symbols= re.compile('[^0-9a-z #+_]')
#STOPWORDS = set(stopwords.words('english'))

def clean_text(text):
    text = text.lower() # lowercase text
    text = space.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.
    text = symbols.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing.
    text = text.replace('x', '')
#    text = re.sub(r'\W+', '', text)
    #text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text
    return text

sample_df['COMMENT']=sample_df['COMMENT'].apply(clean_text)

# Maximimum sequence length
g=[]
for i in sample_df['COMMENT']:
    g.append(i)
maxl = max([len(s) for s in g])
print ('Maximum sequence length in the list of sentences:', maxl)

from keras.preprocessing.text import Tokenizer
# The maximum number of words to be used. (most frequent)
MAX_NB_WORDS = 50000
# Max number of words in each complaint.
MAX_SEQUENCE_LENGTH = 3000
# This is fixed.
EMBEDDING_DIM = 100

tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(sample_df['COMMENT'].values)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

### Tokenize preprocessed data
#Truncate and pad the input sequences so that they are all in the same length for modeling.

from keras.preprocessing.sequence import pad_sequences
X = tokenizer.texts_to_sequences(sample_df['COMMENT'].values)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
print('Shape of data tensor:', X.shape)

def Hebb(list_in):
	list_weight = np.array([X_train])
	list_result = list_in.reshape((9220332,1)) * list_weight
	list_result = list_result.sum(axis=0)
	return classfication(list_result)

def classfication(list_value):
# list_value = [ out_1, out_2 ]
	if(list_value[0] >= 0 and list_value[1] < 0):
		return 'apple'
	elif(list_value[0] < 0 and list_value[1] >= 0):
		return 'banana'

def train(list_in, list_weight):
	pass

### Convert label data
sample_df['Stage'].value_counts()
Y = pd.get_dummies(sample_df['Stage'],columns=sample_df["Stage"]).values
print('Shape of label tensor:', Y.shape)


### Train test split
from sklearn.model_selection import train_test_split
#X_train, X_test, Y_train, Y_test = train_test_split(x,Y_lab, test_size = 0.10, random_state = 42)
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)


"""## Model - AANN"""
import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Dense, Dropout, Activation, Input, SpatialDropout1D, LSTM
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from keras.layers import Embedding
from keras.callbacks import ModelCheckpoint, EarlyStopping

from tensorflow.keras.layers import Dense, Dropout, Activation, Input, SpatialDropout1D, Reshape, Flatten
# use a simple sequential model
model = Sequential()
# this first layer (connecting the input to the output layer) will have fewer than 101 neurons.

#######################################################################################
#     CHANGE THE FIRST PARAMETER (20) BELOW                                           #
#     THIS IS THE VALUE OF m                                                          #
# TRY A VERY SMALL VALUE (DOWN to 1)                                                  #
#     SEE THAT YOU CAN NO LONGER RECOGNIZE THE IMAGE, MUCH LESS FIND THE SALT         #
# TRY A VERY LARGE VALUE (UP TO 101)                                                  #
#     SEE THAT THE ORIGINAL IMAGE IS NEARLY PERFECTLY RECREATED                       #
#######################################################################################
model.add(Dense(units =200, kernel_initializer = 'uniform', activation='relu', input_shape=(3000,)))
model.add(Dense(units =101, kernel_initializer = 'uniform',activation='softmax'))
model.add(Dense(5, kernel_initializer = 'uniform',activation='linear'))

model.compile(loss='mean_squared_error',
              optimizer='adam',metrics = ['accuracy'])
#model.add(Dense(units =5, kernel_initializer = 'uniform',activation='softmax'))
#model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
#model.add(Dense(units =100, kernel_initializer = 'uniform',activation='relu'))
#model.add(Dense(units =3, kernel_initializer = 'uniform',activation='sigmoid'))#padding
#model.add(Dense(units =5, kernel_initializer = 'uniform',activation='softmax'))
#model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# Diplay the model summary
print("model summary")
model.summary()
print("layer shapes of weights and bias arrays")
for x in range(0,len(model.layers) - 1):
    print(x)
    print(model.get_layer(index=x+1).get_weights()[0].shape)
    print(model.get_layer(index=x+1).get_weights()[1].shape)

results = model.fit(X_train, Y_train, batch_size=10, epochs=30)
#model.save

model.save('model_aann.h5')
#pickle.dump(model,open('model.pkl','wb'))

#Model Test on new data

print("Model testing")
new_complaint = ['Where are the news']
seq = tokenizer.texts_to_sequences(new_complaint)
padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)
pred = model.predict(padded)
labels = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4', 'Stage_5']
print(pred, labels[np.argmax(pred)])

'''
## Model - LSTM
import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Dense, Dropout, Activation, Input, SpatialDropout1D, LSTM
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from keras.layers import Embedding
from keras.callbacks import ModelCheckpoint, EarlyStopping

model_ls = Sequential()
model_ls.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1])) # xtrain
model_ls.add(SpatialDropout1D(0.2))
model_ls.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model_ls.add(Dense(5, activation='softmax'))
model_ls.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

epochs = 5
batch_size = 64

history = model_ls.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

model_ls.save('model_lstm.h5')

accr = model_ls.evaluate(X_test,Y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show();

new_complaint = ['Help me']
seq = tokenizer.texts_to_sequences(new_complaint)
padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)
pred = model_ls.predict(padded)
labels = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4', 'Stage_5', 'Stage_6', 'Stage_7', 'Stage_8', 'Stage_9']
print(pred, labels[np.argmax(pred)])

"""## Model - Auto Encoder"""
import numpy as np
import pandas as pd
import tensorflow.keras as keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Dense, Dropout, Activation, Input, SpatialDropout1D, LSTM, BatchNormalization, LeakyReLU
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline 
from keras.layers import Embedding
from keras.callbacks import ModelCheckpoint, EarlyStopping

model_ae = Sequential()
n_inputs = X_train.shape[1]
# define encoder
visible = Input(shape=(n_inputs,))
# encoder level 1
e = Dense(n_inputs*2)(visible)
e = BatchNormalization()(e)
e = LeakyReLU()(e)
# encoder level 2
e = Dense(n_inputs)(e)
e = BatchNormalization()(e)
e = LeakyReLU()(e)
# bottleneck
n_bottleneck = n_inputs
bottleneck = Dense(n_bottleneck)(e)


# define decoder, level 1
d = Dense(n_inputs)(bottleneck)
d = BatchNormalization()(d)
d = LeakyReLU()(d)
# decoder level 2
d = Dense(n_inputs*2)(d)
d = BatchNormalization()(d)
d = LeakyReLU()(d)
# output layer
output = Dense(n_inputs, activation='linear')(d)
# define autoencoder model
model_ae = keras.Model(inputs=visible, outputs=output)
model_ae.save('model_ae.h5')
# compile autoencoder model
model_ae.compile(optimizer='adam', loss='mse')
history = model_ae.fit(X_train, X_train, epochs=5, batch_size=16, verbose=2, validation_data=(X_test,X_test))

pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()
# define an encoder model (without the decoder)
encoder = keras.Model(inputs=visible, outputs=bottleneck)
#plot_model(encoder, 'encoder_no_compress.png', show_shapes=True)
# save the encoder to file
encoder.save('encoder.h5')

# load the model from file
encoder = load_model('encoder.h5')
'''